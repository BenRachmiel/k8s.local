operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: nexus.local:8122/ceph/ceph:v19.2.0
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  network:
    provider: host
  crashCollector:
    disable: false
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "worker1"
      devices:
      - name: "/dev/sdb"  # HDD device
      config:
        storeType: bluestore
    - name: "worker2"
      devices:
      - name: "/dev/sdb"  # NVMe device (adjust to match your NVMe path)
      config:
        storeType: bluestore
    - name: "worker3"
      devices:
      - name: "/dev/sdb"  # NVMe device (adjust to match your NVMe path)
      config:
        storeType: bluestore

# Storage class definitions
cephBlockPools:
# HDD pool (replicated across 3 hosts)
- name: hdd-pool
  spec:
    failureDomain: osd  # Replicate across hosts
    replicated:
      size: 1  # Requires at least 3 OSDs on different hosts
  storageClass:
    enabled: true
    name: rook-ceph-hdd
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    volumeBindingMode: Immediate
    parameters:
      pool: hdd-pool
      imageFormat: "2"
      imageFeatures: layering
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
      csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# Single NVMe storage class (non-replicated, localized to worker2/worker3)
- name: nvme-pool
  spec:
    failureDomain: osd  # Store on a single OSD
    replicated:
      size: 1  # No replication
    # Optional: Use deviceClass to target NVMe OSDs
    # (Requires tagging NVMe OSDs with a device class; see notes below)
  storageClass:
    enabled: true
    name: rook-ceph-nvme-local
    isDefault: false
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    volumeBindingMode: WaitForFirstConsumer
    parameters:
      pool: nvme-pool
      imageFormat: "2"
      imageFeatures: layering
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
      csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# Enable monitoring
monitoring:
  enabled: false

# Dashboard configuration with SSL
dashboard:
  enabled: true
  ssl: true
  ingress:
    enabled: true
    ingressClassName: nginx
    host:
      name: ceph.k8s.local
      path: /
    tls:
    - hosts:
        - ceph.k8s.local
      secretName: ceph-dashboard-cert
    annotations:
      cert-manager.io/cluster-issuer: "default-issuer"
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"

# Enable the Ceph toolbox for debugging
toolbox:
  enabled: true
  resources:
    limits:
      memory: "512Mi"
    requests:
      cpu: "100m"
      memory: "128Mi"

# Additional settings
allowMultipleFilesystems: true
